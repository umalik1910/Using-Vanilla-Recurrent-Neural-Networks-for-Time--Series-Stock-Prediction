# -*- coding: utf-8 -*-
"""IT'S THIS COPY THIS IS FINAL

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_CX1NxVPh1CFDFwvqXsxSktMbW8kL4Y1
"""

#####################################################################################################################
#   CS 4375 Project: RNN for time-series prediction
#   Authors: Jeffrey Muhammad, Utsav Malik, Soham Mukherjee, Shanmugathevan Kanagaraj
#####################################################################################################################

# Libraries we are using
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error



class recurrent_neural_net:
  def __init__(self, dataFile, header=True):
      # Changed this to read xlsx file
      self.raw_input = pd.read_csv(dataFile)
      

  def preprocess(self):
    self.processed_data = self.raw_input
    df = pd.DataFrame(self.processed_data)
    
    # Drop irrelevant columns
    # We will be focusing on the NYA index to create our model. All we need
    # is the close to predict our next closes.
    cols = [0, 1, 2, 3, 4, 6, 7, 8]

    # Remove non-NYA rows
    drop_rows = df[df['Index'] != 'NYA'].index
    df.drop(drop_rows, inplace = True)

    # Drop the selected columns
    df.drop(df.columns[cols],axis=1,inplace=True)

    # Reset index in dataframe
    df.reset_index(drop=True, inplace=True)

    final_data = df.to_numpy().reshape((13947, ))

    final_data = np.array([x for x in final_data[:250]])

    self.processed_data = final_data

    return 0

  def calc_loss(self, y, y_hat):
    return ((y - y_hat)**2) / 2

  def loss_check(self, loss_args, x_arr, y_arr):
    loss = 0.0
    X = x_arr
    Y = y_arr
    T = loss_args[0]
    Input_Layer = loss_args[1]
    Hidden_Layer = loss_args[2]
    Output_Layer = loss_args[3]
    size_hidden = loss_args[4]

    # do a forward pass to get loss
    i = 0
    while i < Y.shape[0]:
        x_temp=X[i]
        y_temp=Y[i]
        # value of previous hidden layer's activation
        prev_act_val = np.zeros((size_hidden,1))
        t = 0
        while t < T:
            # we get a single timestamped value; we'll want to dot product
            # this value through the rest of the layers
            entry = np.zeros(x_temp.shape)
            entry[t] = x_temp[t]              
            Weighted_Input_Layer = np.dot(Input_Layer, entry)
            # pass this through activation
            Weighted_Hidden_Layer = np.dot(Hidden_Layer, prev_act_val)
            # pass this through activation
            remembered_mem = recurrent_neural_net.sigmoid(Weighted_Input_Layer + Weighted_Hidden_Layer)
            Weighted_Output_Layer = np.dot(Output_Layer, remembered_mem)
            prev_act_val = remembered_mem
            t += 1
        # calculate error 
        loss += recurrent_neural_net.calc_loss(y_temp, Weighted_Output_Layer)
        i += 1
    loss = loss / float(Y.shape[0])
    return loss

  def sigmoid(self, x):
    if x.all() >= 0:
      return 1 / (1+np.exp(-x))
    else:
      return np.exp(x)/(1+np.exp(x))
  
  def sigmoid_derivative(self, x):
    sig = recurrent_neural_net.sigmoid(x)
    derivative_sig = sig * (1 - sig)
    return derivative_sig

  def tanh(self, x):
    return np.tanh(x)

  def forward_pass(self, window, x_temp, layers, prev_act_val, Input_Layer, Hidden_Layer, Output_Layer):
    t = 0
    while t < window:
      # we get a single timestamped value; we'll want to dot product
      # this value through the rest of the layers
      entry = np.zeros(x_temp.shape)
      entry[t] = x_temp[t]              
      Weighted_Input_Layer = np.dot(Input_Layer, entry)
      # pass this through activation
      Weighted_Hidden_Layer = np.dot(Hidden_Layer, prev_act_val)
      # pass this through activation
      absolute_mem = Weighted_Input_Layer + Weighted_Hidden_Layer
      remembered_mem = recurrent_neural_net.sigmoid(absolute_mem)
      Weighted_Output_Layer = np.dot(Output_Layer, remembered_mem)
      layers.append({'r':remembered_mem, 'p':prev_act_val})
      prev_act_val = remembered_mem
      t += 1
    return Weighted_Input_Layer, Weighted_Hidden_Layer, Weighted_Output_Layer, absolute_mem, remembered_mem

  # Used to get derivatives to reduce loss
  def backpropagation_through_time(self, backpropagation_ttt, window, upper_clip, lower_clip, x_temp, derivative_Weighted_Output_Layer, layers, Input_Layer, Hidden_Layer, Output_Layer, absolute_mem, Weighted_Input_Layer, Weighted_Hidden_Layer):
      # backward pass
      
      # derivative of each layer at given time stamp, will be used for the loss calculation
      derivative_Input_Layer = np.zeros(Input_Layer.shape)
      derivative_Hidden_Layer = np.zeros(Hidden_Layer.shape)
      derivative_Output_Layer = np.zeros(Output_Layer.shape)

      derivative_Input_Layer_t = np.zeros(Input_Layer.shape)
      derivative_Hidden_Layer_t = np.zeros(Hidden_Layer.shape)
      derivative_Output_Layer_t = np.zeros(Output_Layer.shape)
      
      for t in range(window):

        derivative_Output_Layer_t = np.dot(derivative_Weighted_Output_Layer, np.transpose(layers[t]['r']))
        derivative_sigmoid_v = np.dot(np.transpose(Output_Layer), derivative_Weighted_Output_Layer)        
        derivative_sigmoid = derivative_sigmoid_v
        derivative_addition = recurrent_neural_net.sigmoid_derivative(absolute_mem)                
        derivative_Weighted_Hidden_Layer = derivative_addition * np.ones_like(Weighted_Hidden_Layer)

        derivative_prev_act_val = np.dot(np.transpose(Hidden_Layer), derivative_Weighted_Hidden_Layer)

        derivative_Output_Layer += derivative_Output_Layer_t
        # unfold the network k times
        for k in range(t, max(0, t-backpropagation_ttt-1), -1):
          derivative_sigmoid = derivative_sigmoid_v + derivative_prev_act_val
          derivative_addition = recurrent_neural_net.sigmoid_derivative(absolute_mem) * derivative_sigmoid
          
          derivative_Weighted_Hidden_Layer = derivative_addition * np.ones_like(Weighted_Hidden_Layer)
          derivative_Weighted_Input_Layer = derivative_addition * np.ones_like(Weighted_Input_Layer)
          
          derivative_prev_act_val = np.dot(np.transpose(Hidden_Layer), derivative_Weighted_Hidden_Layer)

          entry = np.zeros(x_temp.shape)
          entry[t] = x_temp[t]

          derivative_Input_Layer_t += np.dot(Hidden_Layer, derivative_prev_act_val)
          derivative_Hidden_Layer_t += np.dot(Input_Layer, entry)

          # Update the weights of the layers  
          derivative_Input_Layer += derivative_Input_Layer_t
          derivative_Hidden_Layer += derivative_Hidden_Layer_t
          

          # clip if the gradients have exploded
          if derivative_Input_Layer.max() > upper_clip:
            derivative_Input_Layer[derivative_Input_Layer > upper_clip] = upper_clip
          if derivative_Hidden_Layer.max() > upper_clip:
            derivative_Hidden_Layer[derivative_Hidden_Layer > upper_clip] = upper_clip
          if derivative_Output_Layer.max() > upper_clip:
            derivative_Output_Layer[derivative_Output_Layer > upper_clip] = upper_clip
          
          # Clip if gradients vanished
          if derivative_Input_Layer.min() < lower_clip:
            derivative_Input_Layer[derivative_Input_Layer < lower_clip] = lower_clip
          if derivative_Hidden_Layer.min() < lower_clip:
            derivative_Hidden_Layer[derivative_Hidden_Layer < lower_clip] = lower_clip
          if derivative_Output_Layer.min() < lower_clip:
            derivative_Output_Layer[derivative_Output_Layer < lower_clip] = lower_clip
      return derivative_Input_Layer, derivative_Hidden_Layer, derivative_Output_Layer


  def make_predictions(self, X, Y, Input_Layer, Hidden_Layer, Output_Layer, hid_dim, window):
      preds = []
      for i in range(Y.shape[0]):
          x= X[i]
          y= Y[i]
          prev_act_val = np.zeros((hid_dim, 1))
          # Forward pass
          for t in range(window):
              Weighted_Input_Layer = np.dot(Input_Layer, x)
              Weighted_Hidden_Layer = np.dot(Hidden_Layer, prev_act_val)
              remembered_mem = recurrent_neural_net.sigmoid(Weighted_Hidden_Layer + Weighted_Input_Layer)
              Weighted_Output_Layer = np.dot(Output_Layer, remembered_mem)
              prev_act_val = remembered_mem

          preds.append(Weighted_Output_Layer)
          
      return np.array(preds)


  def isConverged(self, abs_loss, prev_loss, iterations):
    # if there have been enough iterations and the absolute value of the 
    # loss subtraction is < 100 then we have converged
    return (iterations > 5) and (abs(abs_loss - prev_loss) < 100)

  def rnn(self, params, x_arr, y_arr):
    X = x_arr
    Y = y_arr

    max_iter = params[0]
    lr = params[1]
    window = params[5]
    hid_dim = len(X) - 2*window
    out_dim = 1

    backpropagation_ttt = params[2]
    lower_clip = params[3]
    upper_clip = params[4]

    Input_Layer = np.random.uniform(0, 1, (hid_dim, window))
    Hidden_Layer = np.random.uniform(0, 1, (hid_dim, hid_dim))
    Output_Layer = np.random.uniform(0, 1, (out_dim, hid_dim))

    # Get constants ready for iteration and convergence
    iterations = 1
    prev_loss = 0.0
    loss = 0.0
    loss_arr = []

    # memoize the previous losses for convergence
    memory = []
    memory.append(prev_loss)

    while iterations <= max_iter and recurrent_neural_net.isConverged(loss, prev_loss, iterations) == False:
        
        
        prev_loss = memory[iterations - 1]
        
        # check loss on train
        loss = 0.0

        loss_args = [window, Input_Layer, Hidden_Layer, Output_Layer, hid_dim]

        loss = recurrent_neural_net.loss_check(loss_args, X, Y)

        print('Iteration: ', iterations, ', Cost: ', loss[0][0])
        loss_arr.append(loss[0][0])
        memory.append(loss)

        # train model
        for i in range(Y.shape[0]):
            x_temp= X[i]
            y_actual= Y[i]
        
            layers = []
            prev_act_val = np.zeros((hid_dim, 1))
            
            # forward pass
            Weighted_Input_Layer, Weighted_Hidden_Layer, Weighted_Output_Layer, absolute_mem, remembered_mem = recurrent_neural_net.forward_pass(window, x_temp, layers, prev_act_val, Input_Layer, Hidden_Layer, Output_Layer)
            
            # derivative of prediction - will be used in backprop
            derivative_Weighted_Output_Layer = (Weighted_Output_Layer - y_actual)
            
            derivative_Input_Layer, derivative_Hidden_Layer, derivative_Output_Layer = recurrent_neural_net.backpropagation_through_time(window, backpropagation_ttt, upper_clip, lower_clip, x_temp, derivative_Weighted_Output_Layer,  layers, Input_Layer, Hidden_Layer, Output_Layer, absolute_mem, Weighted_Input_Layer, Weighted_Hidden_Layer)
            # update with the reduced weights
            Input_Layer -= lr * derivative_Input_Layer
            Hidden_Layer -= lr * derivative_Hidden_Layer
            Output_Layer -= lr * derivative_Output_Layer
            

        iterations += 1
    # end of while loop

    # Create table predictions using weights
    preds = recurrent_neural_net.make_predictions(X, Y, Input_Layer, Hidden_Layer, Output_Layer, hid_dim, window)

    it = range(len(loss_arr))

    plt.plot(it, loss_arr, 'b')

    plt.xlabel("Iterations")
    plt.ylabel("Cost")

    plt.show()

    return preds, Input_Layer, Hidden_Layer, Output_Layer, hid_dim, window
  

  def train_evaluate(self):
    # In this section we split into training and testing sets, then run algo
    # We are using close value to predict itself over time
    data = self.processed_data
    
    # Split into subsets, 30 days each
    num_timesteps = 30
    datalen = len(data) - num_timesteps

    # Training sets
    x_train_sets = []
    y_train_sets = []

    # Testing sets
    x_test_sets = []
    y_test_sets = []

    # Now split x_train, y_train into subsets
    for i in range(datalen - num_timesteps):
      endstep = i + num_timesteps
      x_train_sets.append(data[i:endstep])
      y_train_sets.append(data[endstep])

    for i in range(datalen - num_timesteps, datalen):
      endstep = i + num_timesteps
      x_test_sets.append(data[i:endstep])
      y_test_sets.append(data[endstep])


    # Turn the arrays into numpy arrays
    x_train_sets = np.array(x_train_sets)
    y_train_sets = np.array(y_train_sets)
    
    x_test_sets = np.array(x_test_sets)
    y_test_sets = np.array(y_test_sets)

    # could be x_train_sets = np.expand_dims(np.array(x_train_sets), axis=2)
    
    # Expand dimensions of the arrays
    x_train_sets = np.expand_dims(x_train_sets, axis = 2)
    y_train_sets = np.expand_dims(y_train_sets, axis = 1)

    x_test_sets = np.expand_dims(x_test_sets, axis = 2)
    y_test_sets = np.expand_dims(y_test_sets, axis = 1)

    # split into training and testing
    X_train = x_train_sets
    Y_train = y_train_sets

    X_test = x_test_sets
    Y_test = y_test_sets

    '''
    indices = []
      for i in range(0, 1):
        indices.append(i)

    # Columns
    table_cols = ['Activation', 'Max Iterations', 'Learning Rate', '# Hidden Dimensions',
                'Backprop Truncation', 'Min Clip', 'Max Clip', 'Window',
                '% Training Data', '% Testing Data', 'Train RMSE', 'Test RMSE']
    '''

    main_table = pd.DataFrame() #index = indices, columns = table_cols

    Trials = [[30, 0.0001, 5, -5, 5, num_timesteps], [50, 0.0001, 7, -7, 7, num_timesteps],
              [50, 0.0001, 10, -10, 10, num_timesteps], [50, 0.0001, 3, -3, 3, num_timesteps],
              [20, 0.0001, 10, -15, 15, num_timesteps]]

    trial_no = 1
    for trial in Trials:
      print("Trial #: ", trial_no)
      trial_no += 1
      # Parameters to pass to RNN
      params = trial
      preds, Input_Layer, Hidden_Layer, Output_Layer, hid_dim, window = recurrent_neural_net.rnn(params, x_train_sets, y_train_sets)


      # Output the Data Split
      train_split = round((len(Y_train)/(len(Y_train)+len(Y_test))), 2)
      test_split = round((len(Y_test)/(len(Y_train)+len(Y_test))), 2)
      print("Training Data: {}%".format(train_split))
      print("Testing Data: {}%".format(test_split))

      # Output the RMSE
      RMSE_train = math.sqrt(mean_squared_error(Y_train[:, 0], preds[:, 0, 0]))
      train_max = max(Y_train[:, 0])
      train_min = min(Y_train[:, 0])
      normalized_RMSE_train = RMSE_train / (train_max + train_min)
      print("Normalized RMSE for training: ", normalized_RMSE_train)

      # Now make a prediction
      preds = recurrent_neural_net.make_predictions(X_test, Y_test, Input_Layer, Hidden_Layer, Output_Layer, hid_dim, window)

      # Output the RMSE
      RMSE_test = math.sqrt(mean_squared_error(Y_test[:, 0], preds[:, 0, 0]))
      train_max = max(Y_test[:, 0])
      train_min = min(Y_test[:, 0])
      normalized_RMSE_test = RMSE_test / (train_max + train_min)
      print("Normalized RMSE for testing: ", normalized_RMSE_test)



      # Create metrics table and output to log file
      Trial = []
      for i in range(0, 1):
        Trial.append(i)

      # Columns
      table_cols = ['Activation', 'Max Iterations', 'Learning Rate', '# Hidden Dimensions',
                'Backprop Truncation', 'Min Clip', 'Max Clip', 'Window',
                '% Training Data', '% Testing Data', 'Train RMSE', 'Test RMSE']
      table = pd.DataFrame(index = Trial, columns = table_cols)

      train_percent = '{}% of data'.format(round(train_split*100, 2))
      test_percent = '{}% of data'.format(round(test_split*100, 2))
      all_metrics = ['Sigmoid', params[0], params[1], hid_dim, params[2], params[3], params[4],
                    params[5], train_percent, test_percent, round(RMSE_train, 2), round(RMSE_test, 2)]

      # Add values to table
      for i in range(len(all_metrics)):
        table.loc[Trial[0], table_cols[i]] = all_metrics[i]

      print("Metrics table:\n")
      display(table)
      

      # Append to log file

      with open("log.txt", 'a') as f:
        f.write('Trial results:\n')
        dfAsString = table.to_string(header=True, index=False)
        f.write(dfAsString)
        f.write('\n\n')

      # append to dataframe
      if trial_no == 2:
        main_table = table
      else:
        main_table = main_table.append(table, ignore_index = True)

    print("\nFull Metrics table:\n")
    display(main_table)
    return 0


if __name__ == "__main__":
    recurrent_neural_net = recurrent_neural_net('https://gist.githubusercontent.com/jeffmuh/256c52db3563cf26854a7db4c9e31fb3/raw/1e7bdef9d944a8e383ad1c0c69d15e559002f16a/stockdata.csv') # put in path to your file
    recurrent_neural_net.preprocess()
    recurrent_neural_net.train_evaluate()